{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sri-haneesha/Analysis-and-Semantic-Classification-Clustering-of-Newspaper-PDF-Files/blob/main/Copy_of_NLPFIX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45WJ_mf-5Ke-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJd_-6yY5NNi"
      },
      "outputs": [],
      "source": [
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CA85IKAz5P1V"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "df = pd.read_json('News_Category_Dataset_v32.json', lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cvt-ETdk5R22"
      },
      "outputs": [],
      "source": [
        "# Preprocess the data\n",
        "df['category'] = df['category'].replace({\"THE WORLDPOST\": \"WORLDPOST\", \"GREEN\": \"ENVIRONMENT\"})\n",
        "encoder = LabelEncoder()\n",
        "df['categoryEncoded'] = encoder.fit_transform(df['category'])\n",
        "\n",
        "# Function to clean and preprocess text\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
        "    return text\n",
        "\n",
        "df['headline'] = df['headline'].apply(clean_text)\n",
        "df['short_description'] = df['short_description'].apply(clean_text)\n",
        "df['text'] = df['headline'] + \" \" + df['short_description']\n",
        "\n",
        "# Advanced text processing with stopwords removal and lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def advanced_clean_text(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "df['text'] = df['text'].apply(advanced_clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ0CaiSd5Uco"
      },
      "outputs": [],
      "source": [
        "# Selecting only 5 categories: Politics, Entertainment, Sports, Business, and Tech\n",
        "selected_categories = ['POLITICS', 'SPORTS', 'TECH','ENVIRONMENT','CRIME']\n",
        "\n",
        "# Filtering the DataFrame to include only the selected categories\n",
        "df_selected = df[df['category'].isin(selected_categories)]\n",
        "\n",
        "# Limiting the number of records for each category to 200\n",
        "# Ensuring consistent sampling by setting a random state\n",
        "df_sampled = df_selected.groupby('category').apply(lambda x: x.sample(n=min(len(x),500), random_state=42)).reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGx2ReUL5WJI"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    # Normalize the text by lowering the case and removing unwanted characters\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
        "\n",
        "    # Tokenization and removing stopwords and lemmatizing\n",
        "    tokens = text.split()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Rejoin tokens into a single string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df_sampled['text'] = df_sampled['text'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_uV0MHW5ZPh"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into train and test sets\n",
        "# Using a stratified split to maintain the distribution of categories in both training and testing datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_sampled['text'],\n",
        "    df_sampled['categoryEncoded'],\n",
        "    test_size=0.1,\n",
        "    random_state=2020,\n",
        "    stratify=df_sampled['categoryEncoded']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaqpAu0-5ape"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, TFBertModel\n",
        "import tensorflow as tf\n",
        "\n",
        "# Ensure that TensorFlow uses GPU if available for faster processing\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Set TensorFlow to use only the first GPU\n",
        "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    except RuntimeError as e:\n",
        "        # Exception handling if GPU settings fail\n",
        "        print(e)\n",
        "\n",
        "# Load the BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "model = TFBertModel.from_pretrained('bert-large-uncased')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzYDuHHx5cNZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def generate_bert_embeddings(texts, batch_size=32):\n",
        "    embeddings = []\n",
        "\n",
        "    # Process texts in batches to manage memory more efficiently and handle larger datasets\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "        input_ids = tokenizer(batch_texts.tolist(), padding=True, truncation=True, max_length=128, return_tensors=\"tf\")[\"input_ids\"]\n",
        "        outputs = model(input_ids)\n",
        "        batch_embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "        embeddings.append(batch_embeddings)\n",
        "\n",
        "    # Concatenate all batch embeddings into a single array\n",
        "    return tf.concat(embeddings, axis=0).numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdnlKcSE5e8c"
      },
      "outputs": [],
      "source": [
        "# Generate BERT embeddings for train and test data using the enhanced function\n",
        "# It's important to determine the optimal batch size based on your system's GPU memory limits.\n",
        "optimal_batch_size = 16  # Adjust based on your system's configuration and available memory\n",
        "\n",
        "X_train_bert_embeddings = generate_bert_embeddings(X_train, batch_size=optimal_batch_size)\n",
        "X_test_bert_embeddings = generate_bert_embeddings(X_test, batch_size=optimal_batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDk76FX1L-in"
      },
      "outputs": [],
      "source": [
        "X_train_bert_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPREFDmXMCa8"
      },
      "outputs": [],
      "source": [
        "X_test_bert_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbw9TB9p5hDU"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Exception handling to manage potential issues during scaling\n",
        "try:\n",
        "    # Scale the input features for SVM and KNN\n",
        "    X_train_scaled = scaler.fit_transform(X_train_bert_embeddings)\n",
        "    X_test_scaled = scaler.transform(X_test_bert_embeddings)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while scaling the features: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Yzkx-mF5jpg"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import joblib\n",
        "\n",
        "# SVM hyperparameters\n",
        "svm_params = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto', 0.001, 0.01, 0.1]}\n",
        "svm_classifier = GridSearchCV(SVC(kernel='linear'), svm_params, cv=3, verbose=2, n_jobs=joblib.cpu_count() - 1)\n",
        "\n",
        "# Training SVM classifier with hyperparameter optimization\n",
        "try:\n",
        "    svm_classifier.fit(X_train_scaled, y_train)\n",
        "    print(f\"Best parameters found: {svm_classifier.best_params_}\")\n",
        "    print(f\"Best cross-validation score: {svm_classifier.best_score_:.2f}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while training SVM: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6qMhPXo5kil"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Predict using optimized SVM classifier\n",
        "svm_predictions = svm_classifier.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy and other performance metrics\n",
        "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
        "precision, recall, fscore, _ = precision_recall_fscore_support(y_test, svm_predictions, average='weighted')\n",
        "\n",
        "# Display the results\n",
        "print(\"SVM Accuracy:\", svm_accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F-Score:\", fscore)\n",
        "print(\"Best SVM Parameters:\", svm_classifier.best_params_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6eSysJ65nm7"
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters for KNN\n",
        "knn_params = {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']}\n",
        "knn_classifier = GridSearchCV(KNeighborsClassifier(), knn_params, cv=3, verbose=2, n_jobs=joblib.cpu_count() - 1)\n",
        "\n",
        "# Training KNN classifier with hyperparameter optimization\n",
        "try:\n",
        "    knn_classifier.fit(X_train_scaled, y_train)\n",
        "    print(f\"Best KNN Parameters: {knn_classifier.best_params_}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during KNN training: {e}\")\n",
        "\n",
        "# Predict using optimized KNN classifier\n",
        "try:\n",
        "    knn_predictions = knn_classifier.predict(X_test_scaled)\n",
        "    knn_accuracy = accuracy_score(y_test, knn_predictions)\n",
        "    print(\"KNN Accuracy:\", knn_accuracy)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during KNN prediction: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUMp5Ngc5pQa"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Train RandomForestClassifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "rf_classifier.fit(X_train_bert_embeddings, y_train)\n",
        "\n",
        "# Predict using RandomForestClassifier\n",
        "rf_predictions = rf_classifier.predict(X_test_bert_embeddings)\n",
        "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
        "rf_precision = precision_score(y_test, rf_predictions, average='weighted')\n",
        "rf_recall = recall_score(y_test, rf_predictions, average='weighted')\n",
        "rf_f1_score = f1_score(y_test, rf_predictions, average='weighted')\n",
        "\n",
        "# Display the results\n",
        "print(\"RandomForestClassifier Accuracy:\", rf_accuracy)\n",
        "print(\"Precision:\", rf_precision)\n",
        "print(\"Recall:\", rf_recall)\n",
        "print(\"F1 Score:\", rf_f1_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_btXQ3_9miH"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the models to be compared\n",
        "models = {\n",
        "    \"RandomForestClassifier\": RandomForestClassifier(n_estimators=100, random_state=0),\n",
        "    \"GradientBoostingClassifier\": GradientBoostingClassifier(n_estimators=100, random_state=0),\n",
        "    \"LogisticRegression\": LogisticRegression(max_iter=1000, random_state=0),\n",
        "    \"SVM\": SVC(kernel='linear', random_state=0),\n",
        "    \"KNeighborsClassifier\": KNeighborsClassifier(n_neighbors=5)\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "accuracies = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_bert_embeddings, y_train)  # Train model\n",
        "    predictions = model.predict(X_test_bert_embeddings)  # Predict on the test set\n",
        "    accuracies[name] = accuracy_score(y_test, predictions)  # Calculate accuracy\n",
        "\n",
        "# Print accuracies for each model\n",
        "for name, accuracy in accuracies.items():\n",
        "    print(f\"{name} Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Visualization of the accuracies\n",
        "plt.figure(figsize=(10, 6))\n",
        "model_names = list(accuracies.keys())\n",
        "model_accuracies = list(accuracies.values())\n",
        "plt.bar(model_names, model_accuracies, color='skyblue')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Comparison of Model Accuracies')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtDQ5spV9miI"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming you've already split the data and have X_train_bert_embeddings, X_test_bert_embeddings, y_train, y_test\n",
        "\n",
        "# Gradient Boosting\n",
        "gbm_classifier = GradientBoostingClassifier(n_estimators=100, random_state=0)\n",
        "gbm_classifier.fit(X_train_bert_embeddings, y_train)\n",
        "gbm_predictions = gbm_classifier.predict(X_test_bert_embeddings)\n",
        "\n",
        "# RandomForest\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "rf_classifier.fit(X_train_bert_embeddings, y_train)\n",
        "rf_predictions = rf_classifier.predict(X_test_bert_embeddings)\n",
        "\n",
        "# Logistic Regression\n",
        "lr_classifier = LogisticRegression(max_iter=1000, random_state=0)\n",
        "lr_classifier.fit(X_train_bert_embeddings, y_train)\n",
        "lr_predictions = lr_classifier.predict(X_test_bert_embeddings)\n",
        "\n",
        "# SVM\n",
        "svm_classifier = SVC(kernel='linear', random_state=0)\n",
        "svm_classifier.fit(X_train_bert_embeddings, y_train)\n",
        "svm_predictions = svm_classifier.predict(X_test_bert_embeddings)\n",
        "\n",
        "# KNeighbors\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_classifier.fit(X_train_bert_embeddings, y_train)\n",
        "knn_predictions = knn_classifier.predict(X_test_bert_embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WmEZkYS9miK"
      },
      "outputs": [],
      "source": [
        "rom sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Organize predictions into a dictionary\n",
        "model_predictions = {\n",
        "    \"RandomForest\": rf_predictions,\n",
        "    \"GradientBoosting\": gbm_predictions,\n",
        "    \"LogisticRegression\": lr_predictions,\n",
        "    \"SVM\": svm_predictions,\n",
        "    \"KNN\": knn_predictions\n",
        "}\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(3, 2, figsize=(12, 18))\n",
        "fig.delaxes(axes[2,1])  # Remove the last subplot if odd number of models\n",
        "\n",
        "# Plot confusion matrices\n",
        "for ax, (name, predictions) in zip(axes.flatten(), model_predictions.items()):\n",
        "    conf_mat = confusion_matrix(y_test, predictions)\n",
        "    # Find the top 5 labels/categories with the highest true positive counts\n",
        "    top_labels = conf_mat.diagonal().argsort()[-5:][::-1]  # Get indices of top 4 labels by true positives\n",
        "    filtered_conf_mat = conf_mat[top_labels, :][:, top_labels]  # Filter matrix to only include top 4 labels\n",
        "    sns.heatmap(filtered_conf_mat, annot=True, fmt='d', ax=ax, cmap=\"Blues\")\n",
        "    ax.set_title(f'{name} Confusion Matrix')\n",
        "    ax.set_xlabel('Predicted Labels')\n",
        "    ax.set_ylabel('True Labels')\n",
        "    # Adjust x and y ticks to show only top labels\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
        "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}